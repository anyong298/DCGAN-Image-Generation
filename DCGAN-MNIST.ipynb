{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep Convolutional Generative Adversarial Nets (DCGAN) - generate MNIST image samples</h2>\n",
    "\n",
    "<p>Original DCGAN paper: <a href='https://arxiv.org/pdf/1511.06434.pdf'>https://arxiv.org/pdf/1511.06434.pdf</a></p>\n",
    "\n",
    "<p>The model structure is borrowed from and modified based on Arthur Juliani's implementation: <a href='https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39#.4kvc6juwc'>https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39#.4kvc6juwc</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "import scipy.misc\n",
    "import time\n",
    "from images2gif import writeGif\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "np.random.seed(1) \n",
    "tf.set_random_seed(1)\n",
    "\n",
    "# config\n",
    "pre_train_epochs = 1 # num of epochs to pre-train discriminator\n",
    "training_epochs = 4001 # num of epochs to train discriminator and generator jointly\n",
    "batch_size = 40 # batch size for training the model, must be larger than batch_size_sample\n",
    "\n",
    "# for the generated images\n",
    "batch_size_sample = 36 # number of images to be generated for viewing the training progress\n",
    "# the generated images will be concatenated, the product of the following must equal to batch_size_sample\n",
    "sample_num_rows = 6 # how many rows to have\n",
    "sample_num_columns = 6 # how many columns to have\n",
    "\n",
    "leakiness = 0.2 # degree of leakiness used in leaky ReLU\n",
    "alpha = 0.0002 # base learning rate\n",
    "beta1 = 0.5 # the fraction factor used in the first momentum term from Adam optimizer\n",
    "k = 1 # this is number of times to update discriminator for every time generator is updated in each epoch\n",
    "logs_path = \"/tmp/dcgan_mnist\" # directory to save the training log to\n",
    "train_sample_directory = './dcgan_mnist/train_sample/' # directory to save the generated images during training\n",
    "model_directory = './dcgan_mnist/models' # directory to save trained model\n",
    "sample_directory = './dcgan_mnist/generated_sample/' # directory to save the generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download and read the MNIST data\n",
    "# Each value represent a pixel intensity ranges from 0 - 1 \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def leaky_relu(x, leak=leakiness, name=\"leaky_relu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        return tf.maximum(leak*x, x)\n",
    "\n",
    "# get a batch of real images, change size from 28*28 to 32*32, return the processed batch\n",
    "def get_x(batch_size):\n",
    "    x,_ = mnist.train.next_batch(batch_size) # values range from [0, 1]\n",
    "    x = (np.reshape(x,[batch_size,28,28,1]) - 0.5) * 2.0 # change value range to [-1, 1] for better leaky relu activation\n",
    "\n",
    "    # Add padding to the images so they are 32*32\n",
    "    # (no change in batch_size, add image width by 2 on the left and 2 on the right, add image hight by 2 on the top\n",
    "    # and 2 on the bottom, no change in image channel)\n",
    "    # padding values are -1\n",
    "    x = np.lib.pad(x, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) \n",
    "\n",
    "    return x\n",
    "        \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "\n",
    "# wrapper function\n",
    "def save_images(images, size, image_path):\n",
    "    images = inverse_transform(images) # rescale image pixel values from [-1, 1] to [0, 1]\n",
    "    concat_img = concat(images, size) # concatenate individual images to a grid of images\n",
    "    return scipy.misc.imsave(image_path, concat_img)\n",
    "\n",
    "# change values from [-1, 1] to [0, 1]\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "# concatenate individual images to a grid of images\n",
    "def concat(images, size):\n",
    "    # get height and width for a single image generated, e.g. 32 * 32\n",
    "    height, width = images.shape[1], images.shape[2] \n",
    "    \n",
    "    # placeholder for a concatenated img, which have \n",
    "    # img_height = image height * num images vertically, e.g. 32 * 5\n",
    "    # img_width = image width * num images horizontally, e.g. 32 * 5\n",
    "    img = np.zeros((height * size[0], width * size[1])) \n",
    "\n",
    "    # loop through each image\n",
    "    for index, image in enumerate(images):\n",
    "        j = index / size[0] # image row index\n",
    "        i = index % size[1] # image column index\n",
    "        img[j*height:j*height+height, i*width:i*width+width] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save orginal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(train_sample_directory):\n",
    "    os.makedirs(train_sample_directory)\n",
    "\n",
    "batch = mnist.train.next_batch(batch_size_sample)[0] # get a sample batch of real images\n",
    "batch = np.reshape(batch,[batch_size_sample,28,28]) # reshape images\n",
    "save_images(images=batch, size=[sample_num_rows,sample_num_columns], image_path=train_sample_directory+'/fig_original.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The generator network takes a noise vector z and return a 32 * 32 image. \n",
    "# We use tensorflow slim to perform fractionally-strided convolution, batch normalization and ReLU activation\n",
    "# for each layer except the last layer\n",
    "def generator(z):\n",
    "    \n",
    "    # Turn z into the first tensor + batch norm + relu\n",
    "    # Creates a fully connected weight matrix, which is multiplied by the mini-batch of z vectors to \n",
    "    # produce a hidden layer with 4*4*64 hidden nodes\n",
    "    zP = slim.fully_connected(inputs=z,num_outputs=4*4*64,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_init',weights_initializer=initializer)\n",
    "    # Transform the flat 4*4*64 layer into a tensor, whose kernel size is 4*4 and 64 kernels in total\n",
    "    # -1 stands for the mini-batch size that's to be computed\n",
    "    zCon = tf.reshape(zP,[-1,4,4,64])\n",
    "    \n",
    "    # Perform fractionally-strided convolution/deconvolution + batch norm + relu\n",
    "    # num_outputs: number of output filters/activation maps\n",
    "    # kernel_size: [kernel_height, kernel_width]\n",
    "    # stride: [stride_height, stride_width], in this case it is generating four pixels out of every pixel, this gives fractionally-strided convolution\n",
    "    # output size batch_size*8*8*32\n",
    "    gen1 = slim.convolution2d_transpose(\\\n",
    "        inputs=zCon,num_outputs=32,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    \n",
    "    # output size batch_size*16*16*16\n",
    "    gen2 = slim.convolution2d_transpose(\\\n",
    "        inputs=gen1,num_outputs=16,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    # output size batch_size*32*32*1\n",
    "    g_out = slim.convolution2d_transpose(\\\n",
    "        inputs=gen2,num_outputs=1,kernel_size=[5,5],stride=[2,2],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_output', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The discriminator network takes a 32*32 image and return a probability of whether it is real or generated\n",
    "# We use tensorflow slim to perform standard convolution, batch normalization and leaky ReLU activation\n",
    "# for each layer\n",
    "def discriminator(image, reuse=False):\n",
    "    # Creates 16 4*4 filters to convolve on the mini-batch of 32*32*1 images, also perform batch norm + leaky ReLU activation\n",
    "    # Note that no pooling is performed\n",
    "    # stride here calculates one pixel out of every 2*2 pixels, this gives strided convolution that will shrink the image as a substitute for pooling\n",
    "    # Set reuse=True allows discriminator to evaluate both real samples and generated samples \n",
    "    # Outputs size batch_size*16*16*16 \n",
    "    dis1 = slim.convolution2d(inputs=image,num_outputs=16,kernel_size=[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=leaky_relu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    \n",
    "    # outputs size batch_size*8*8*32 \n",
    "    dis2 = slim.convolution2d(inputs=dis1,num_outputs=32,kernel_size=[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=leaky_relu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    # outputs size batch_size*4*4*64 \n",
    "    dis3 = slim.convolution2d(inputs=dis2,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=leaky_relu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    \n",
    "    # flatten the tensor to [batch_size, 4*4*64]\n",
    "    dis_flat = slim.flatten(dis3)\n",
    "    \n",
    "    # create a fully connect layer with dis_flat and just one node in the output layer\n",
    "    # note there's no batch normalization at this layer\n",
    "    # outputs size batch_size*1 \n",
    "    d_out = slim.fully_connected(inputs=dis_flat,num_outputs=1,\\\n",
    "        activation_fn=tf.nn.sigmoid, reuse=reuse, scope='d_output', weights_initializer=initializer)\n",
    "    \n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting two networks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_size = 100 # size of initial noise vector that will be used for generator\n",
    "\n",
    "# initialize all parameters of the networks\n",
    "# weights were initialized from a zero-centered Normal distribution with standard deviation 0.02\n",
    "# tf.truncated_normal returns random values from a normal distribution and made sure no value exceeds 2 std\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "# placeholders for inputs into the generator and discriminator, respectively.\n",
    "z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32, name='z_vectors') \n",
    "x_vector = tf.placeholder(shape=[batch_size,32,32,1],dtype=tf.float32, name='real_images') \n",
    "\n",
    "# ---- Pre-training ----\n",
    "# the discriminator should output probability=1 for all the training images\n",
    "train_labels=tf.constant(1.0,shape=(batch_size,1), name='pre_train_labels')\n",
    "# feed images to the discriminator and return the predicted probability\n",
    "d_pre_output = discriminator(x_vector)\n",
    "summary_pre_d_x_hist = tf.histogram_summary(\"pre_train_d_prob_x\", d_pre_output)\n",
    "\n",
    "d_pre_loss=tf.reduce_mean(tf.square(d_pre_output-train_labels))\n",
    "summary_pre_d_loss = tf.scalar_summary(\"pre_train_d_loss\", d_pre_loss)\n",
    "# ---- end of Pre-training ----\n",
    "\n",
    "\n",
    "# ---- DCGAN ----\n",
    "g_output = generator(z_vector) # generated mini-batch of images from noisy z vectors\n",
    "d_output_x = discriminator(x_vector,reuse=True) # probabilities for real images\n",
    "d_output_x = tf.maximum(tf.minimum(d_output_x, 0.99), 0.01) # avoid inf and -inf\n",
    "summary_d_x_hist = tf.histogram_summary(\"d_prob_x\", d_output_x)\n",
    "\n",
    "d_output_z = discriminator(g_output,reuse=True) # probabilities for generated images\n",
    "d_output_z = tf.maximum(tf.minimum(d_output_z, 0.99), 0.01) # avoid inf and -inf\n",
    "summary_d_z_hist = tf.histogram_summary(\"d_prob_z\", d_output_z)\n",
    "\n",
    "d_loss = -tf.reduce_mean(tf.log(d_output_x) + tf.log(1-d_output_z)) # loss for discriminator\n",
    "summary_d_loss = tf.scalar_summary(\"d_loss\", d_loss)\n",
    "\n",
    "g_loss = -tf.reduce_mean(tf.log(d_output_z)) # loss for generator\n",
    "summary_g_loss = tf.scalar_summary(\"g_loss\", g_loss)\n",
    "\n",
    "# the following parameter indices may change if the network structure changes\n",
    "para_d = tf.trainable_variables()[:7] # parameters for discriminator\n",
    "para_g = tf.trainable_variables()[7:] # parameters for generator\n",
    "\n",
    "# only update parameters in discriminator during pre-training\n",
    "pre_optimizer = tf.train.AdamOptimizer(learning_rate=alpha,beta1=beta1).minimize(d_pre_loss,var_list=para_d)\n",
    "# only update the weights for the discriminator network\n",
    "optimizer_op_d = tf.train.AdamOptimizer(learning_rate=alpha,beta1=beta1).minimize(d_loss,var_list=para_d)\n",
    "# only update the weights for the generator network\n",
    "optimizer_op_g = tf.train.AdamOptimizer(learning_rate=alpha,beta1=beta1).minimize(g_loss,var_list=para_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DCGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a log folder and save the graph structure, do this before training\n",
    "g_writer = tf.train.SummaryWriter(logs_path + '/generator', graph=tf.get_default_graph())\n",
    "d_writer = tf.train.SummaryWriter(logs_path + '/discriminator')\n",
    "\n",
    "# saver saves and loads variables of the model to and from checkpoints, \n",
    "# which are binary files that maps variable names to tensor values\n",
    "saver = tf.train.Saver(max_to_keep=10) # maximally save 10 checkpoints\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    # variables need to be initialized before we can use them\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    #print [v.name for v in tf.trainable_variables()] # print all variable names\n",
    "    \n",
    "    \n",
    "    # -------- pre-train discriminator --------\n",
    "    start = time.time()\n",
    "    for epoch in range(pre_train_epochs):\n",
    "        x = get_x(batch_size) # get a batch of real images\n",
    "        \n",
    "        d_pre_summary_merge = tf.merge_summary([summary_pre_d_loss, summary_pre_d_x_hist])\n",
    "        summary_pre_d,_=sess.run([d_pre_summary_merge,pre_optimizer], feed_dict={x_vector: x})\n",
    "        \n",
    "        d_writer.add_summary(summary_pre_d, epoch) # add loss summary to tensorboard\n",
    "        time_lapse = time.time()-start\n",
    "        start = time.time()\n",
    "        print \"pre-train epoch: \", epoch,\", time spent: %.2fs\" % time_lapse\n",
    "   \n",
    "    print \"pre-train done.\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------- jointly training discriminator and generator --------\n",
    "    start = time.time()\n",
    "    \n",
    "    # z noise vector that will be used to generate image to check the training progress\n",
    "    z_sample = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32)\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        x = get_x(batch_size) # get a batch of real images\n",
    "        # mini-batch of noise data from [-1, 1]\n",
    "        z = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32)\n",
    "        \n",
    "        for i in range(k):\n",
    "            # Update the discriminator\n",
    "            d_summary_merge = tf.merge_summary([summary_d_loss, summary_d_x_hist,summary_d_z_hist])\n",
    "            summary_d,_ = sess.run([d_summary_merge,optimizer_op_d],feed_dict={z_vector:z, x_vector:x}) \n",
    "        \n",
    "        # Update the generator\n",
    "        _ = sess.run([optimizer_op_g],feed_dict={z_vector:z}) \n",
    "        summary_g,_ = sess.run([summary_g_loss,optimizer_op_g],feed_dict={z_vector:z})\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            d_writer.add_summary(summary_d, epoch) # add loss summary to tensorboard\n",
    "            g_writer.add_summary(summary_g, epoch)\n",
    "        \n",
    "        if epoch % 200 == 0 or epoch in [5,20,40,60]:\n",
    "            time_lapse = time.time()-start\n",
    "            start = time.time()\n",
    "            \n",
    "            print \"DCGAN epoch: \", epoch,\", time spent: %.2fs\" % time_lapse\n",
    "            g_images = sess.run(g_output,feed_dict={z_vector:z_sample}) # get a generated image\n",
    "            \n",
    "            # make a directory for generated images\n",
    "            if not os.path.exists(train_sample_directory):\n",
    "                os.makedirs(train_sample_directory)\n",
    "            \n",
    "            #Save sample generator images for viewing training progress.\n",
    "            save_images(images = np.reshape(g_images[0:batch_size_sample],[batch_size_sample,32,32]),\\\n",
    "                        size = [sample_num_rows,sample_num_columns], image_path = train_sample_directory+'/'+str(epoch)+'.png')\n",
    "        \n",
    "        if epoch in [0,5,20,60,100,\n",
    "                    500,1000,2000,3000,4000]:\n",
    "            # make a directory for trained models\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            \n",
    "            # save the trained model at different epoch\n",
    "            saver.save(sess, save_path = model_directory + '/' + str(epoch) + '.cptk')\n",
    "    print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine sample images generated from training and save it as GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_names = sorted([int(f[:-4]) for f in os.listdir(train_sample_directory) if f.endswith('.png')]) # get image filenames\n",
    "file_names = [str(f)+'.png' for f in file_names] # add the format suffix at the end of the filenames\n",
    "images = [Image.open(train_sample_directory+f) for f in file_names] # turn into image instances\n",
    "filename = \"train_samples.gif\"\n",
    "\n",
    "if not os.path.exists(train_sample_directory):\n",
    "            os.makedirs(train_sample_directory)\n",
    "writeGif(train_sample_directory+filename, images, duration=0.1) # combine images to gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use the trained generator to generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print 'Loading models...might take a minute'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # reload the trained model.\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    models = ckpt.all_model_checkpoint_paths\n",
    "    #print 'all saved models: ',ckpt.all_model_checkpoint_paths\n",
    "    \n",
    "    # z noise vector that will be used to generate image to check the training progress\n",
    "    z_sample = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32)\n",
    "    \n",
    "    # loop through each checkpoint and use models to generate images at different training stages\n",
    "    for index, model in enumerate(models):\n",
    "        saver.restore(sess, save_path=model)\n",
    "\n",
    "        g_images = sess.run(g_output,feed_dict={z_vector:z_sample}) # get a generated image\n",
    "\n",
    "        # make a directory for generated images\n",
    "        if not os.path.exists(sample_directory):\n",
    "            os.makedirs(sample_directory)\n",
    "\n",
    "        #Save generated sample images\n",
    "        save_images(images = np.reshape(g_images[0:batch_size_sample],[batch_size_sample,32,32]),\\\n",
    "                    size = [sample_num_rows,sample_num_columns], image_path = sample_directory+'/'+str(index)+'.png')\n",
    "\n",
    "        print \"Model \"+str(index),': ',model\n",
    "        \n",
    "time_lapse = time.time()-start\n",
    "print \" Time spent: %.2fs\" % time_lapse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine sample images generated from checkpoints and save it as GIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Restart the kernel if not working..</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_names = sorted([int(f[:-4]) for f in os.listdir(sample_directory) if f.endswith('.png')]) # get image filenames\n",
    "file_names = [str(f)+'.png' for f in file_names] # add the format suffix at the end of the filenames\n",
    "images = [Image.open(sample_directory+f) for f in file_names] # turn into image instances\n",
    "filename = \"generated_samples.gif\"\n",
    "\n",
    "if not os.path.exists(sample_directory):\n",
    "            os.makedirs(sample_directory)\n",
    "writeGif(sample_directory+filename, images, duration=0.1) # combine images to gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_directory = './dcgan_mnist_4k/models/' # directory to save trained model\n",
    "sample_inter_directory = './dcgan_mnist_4k/interpolation/' # directory to save the generated images from linear interpolation\n",
    "# make a directory for generated images\n",
    "if not os.path.exists(sample_inter_directory):\n",
    "    os.makedirs(sample_inter_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_interpolation(images, size):\n",
    "    # get height and width for a single image generated, e.g. 64 * 64\n",
    "    height, width = images.shape[1], images.shape[2] \n",
    "    \n",
    "    # placeholder for a concatenated img, which have \n",
    "    # img_height = image height * num images vertically, e.g. 64 * 5\n",
    "    # img_width = image width * num images horizontally, e.g. 64 * 5\n",
    "    img = np.zeros((height * size[0], width * size[1],3))\n",
    "    #print img.shape\n",
    "\n",
    "    # loop through each image\n",
    "    for index, image in enumerate(images):\n",
    "        j = index / size[0] # image row index\n",
    "        i = index % size[1] # image column index\n",
    "        img[j*height:j*height+height, i*width:i*width+width,:] = image\n",
    "        \n",
    "        \n",
    "# spherical interpolation between pointA and pointB, each is a ndarray\n",
    "# val is a value between [0,1], where 0 returns pointA, 1 returns pointB\n",
    "def slerp(val, pointA, pointB):\n",
    "    omega = np.arccos(np.clip(np.dot(pointA/np.linalg.norm(pointA), pointB/np.linalg.norm(pointB)), -1, 1))\n",
    "    so = np.sin(omega)\n",
    "    if so == 0:\n",
    "        return (1.0-val) * pointA + val * pointB # L'Hopital's rule/LERP\n",
    "    return np.sin((1.0-val)*omega) / so * pointA + np.sin(val*omega) / so * pointB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# size of initial noise vector that will be used for generator\n",
    "z_size = 100 \n",
    "\n",
    "# placeholders for inputs into the generator\n",
    "# shape[0] is None means it could take any integer value\n",
    "z_vector = tf.placeholder(shape=[None,z_size],dtype=tf.float32, name='z_vectors')\n",
    "\n",
    "# initialize all parameters of the networks\n",
    "# weights were initialized from a zero-centered Normal distribution with standard deviation 0.02\n",
    "# tf.truncated_normal returns random values from a normal distribution and made sure no value exceeds 2 std\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "# generated mini-batch of images from noisy z vectors\n",
    "g_output = generator(z_vector)\n",
    "\n",
    "print 'Loading MNIST models...might take a minute'\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most trained model:  ./dcgan_mnist_4k/models/4000.cptk\n",
      "spherical interpolation\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # reload the trained model.\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    model = ckpt.model_checkpoint_path\n",
    "    print 'most trained model: ', model\n",
    "\n",
    "    # z noise vector that will be used to generate image to check the training progress\n",
    "    z_sample = np.random.uniform(-1.0,1.0,size=[batch_size_sample,z_size]).astype(np.float32)\n",
    "\n",
    "    # restore model variables\n",
    "    saver.restore(sess, save_path=model)\n",
    "\n",
    "    # linear interpolation\n",
    "    z_sample_one = z_sample[:2]\n",
    "    #print z_sample_one.shape\n",
    "\n",
    "    # set fixed input, interpolate from 7 to 9\n",
    "    sample1='''-0.13812219  0.72763479 -0.12574646 -0.0182618   0.61583346  0.46916214\n",
    "   0.14667514  0.21866253  0.00216453  0.47648311 -0.77202815  0.68840188\n",
    "  -0.16999102 -0.54367113 -0.50641257 -0.3653219  -0.84792823  0.75601202\n",
    "   0.08892749  0.4203513   0.03922871 -0.69616371  0.92312521 -0.28905192\n",
    "   0.72666872  0.17526712 -0.14319344  0.25685716 -0.26958612 -0.43560231\n",
    "  -0.83336276 -0.45225969  0.25468764 -0.66877216 -0.111047    0.5109219\n",
    "   0.36932391  0.63024729 -0.50508118  0.64383519  0.42127076  0.74423504\n",
    "  -0.5183841   0.13616003  0.41404498 -0.86310548 -0.77108097 -0.80045879\n",
    "  -0.78332216  0.91736335 -0.08512668 -0.86796314 -0.2598148   0.29403165\n",
    "   0.02679804  0.62187326  0.61458081  0.7741124   0.86198151 -0.45453411\n",
    "  -0.60225844  0.22483984 -0.48502383 -0.8646636  -0.08629571  0.5238927\n",
    "   0.306025    0.85115772 -0.29717201 -0.44797084  0.49626905  0.41648152\n",
    "   0.04967806  0.58770573 -0.12688731 -0.73914015 -0.31743556 -0.5468775\n",
    "  -0.6892097  -0.80815262 -0.69773364  0.95089406 -0.3983663   0.8524965\n",
    "   0.28033748 -0.63092893  0.25420371  0.16844808  0.65201157 -0.18057719\n",
    "   0.88261253  0.73636746 -0.47242001  0.35281563 -0.64418107 -0.66982579\n",
    "  -0.33878803 -0.75849366 -0.23950006 -0.45955518'''\n",
    "    sample2='''0.85101515  0.51374036  0.01500881 -0.42103535  0.24928139 -0.24745837\n",
    "  -0.12437476 -0.43647423  0.37256166  0.25429964 -0.86342484 -0.50074399\n",
    "   0.36094844  0.04752741  0.7953878   0.94379628  0.39963263 -0.40967974\n",
    "  -0.95484734  0.28475836  0.62765479 -0.77327323  0.10163277 -0.64927554\n",
    "   0.61540842 -0.13411519 -0.44134077  0.07410253  0.21522656  0.3721008\n",
    "   0.61486959 -0.57751054  0.81080121 -0.20757794  0.6915518   0.50987828\n",
    "  -0.91912782  0.89409041  0.08876665  0.22706899  0.47728395 -0.88446796\n",
    "   0.94171816  0.49114835 -0.45817032 -0.38677561  0.98655891  0.13367338\n",
    "   0.33596939 -0.10619202  0.64132112 -0.74780822  0.67132586  0.24326575\n",
    "   0.3466444  -0.48140165  0.91027969  0.35116872 -0.76117933 -0.32121259\n",
    "  -0.65382147  0.86838818 -0.44831353 -0.21466644 -0.24085194  0.89692616\n",
    "   0.91922706 -0.73745388  0.1747136   0.80469757  0.18904562 -0.19690935\n",
    "  -0.28799164 -0.242294    0.98938328 -0.61107469  0.6935581  -0.38045514\n",
    "  -0.74832153 -0.39835727 -0.15829515  0.68855262  0.05665513  0.25415811\n",
    "   0.85488611 -0.77779335  0.3220959  -0.97219348 -0.49998266  0.60628974\n",
    "  -0.54538608 -0.32535785  0.27962732 -0.54508322  0.74605769 -0.84807205\n",
    "  -0.34984127 -0.73864782  0.48457471  0.48459283'''\n",
    "    # turn the string above into an array. sample1 represents a point in a 100-dimensional space\n",
    "    sample1 = np.array([float(i.rstrip()) for i in sample1.split(' ') if i != ''])  \n",
    "    sample2 = np.array([float(i.rstrip()) for i in sample2.split(' ') if i != ''])\n",
    "    #z_sample_one[1] = np.zeros(100) # keep the second digit constant\n",
    "\n",
    "    num_inter = 7 # number of interpolated images\n",
    "    \n",
    "    '''\n",
    "    # linear interpolation between sample1 and sample2  \n",
    "    print 'linear interpolation'\n",
    "    for i in range(num_inter):\n",
    "        if i == 0: # interpolation start with point A\n",
    "            z_sample_one[0] = sample1\n",
    "        elif i == num_inter-1: # interpolation end with point B\n",
    "            z_sample_one[0] = sample2\n",
    "        else:\n",
    "            z_sample_one[0] += (sample2-sample1)/num_inter # linear interpolate from point A to B\n",
    "        #print z_sample_one,'\\n'\n",
    "\n",
    "        g_images = sess.run(g_output,feed_dict={z_vector:z_sample_one})\n",
    "        g_image = g_images[0] # only retrieve the first returned image\n",
    "        \n",
    "        \n",
    "        # save interpolated image\n",
    "        image = inverse_transform(g_image)\n",
    "        image = np.reshape(image,[32,32])\n",
    "        image = Image.fromarray(image)\n",
    "        scipy.misc.imsave(sample_inter_directory+'/'+str(i)+'.png', image)\n",
    "        \n",
    "    '''\n",
    "    print 'spherical interpolation'\n",
    "    # spherical interpolation between pointA and pointB, each is a ndarray\n",
    "    # val is a value between [0,1], where 0 returns pointA, 1 returns pointB\n",
    "    for i, value in enumerate(np.linspace(0,1,num_inter)):\n",
    "        z_sample_one[0] = slerp(val=value, pointA=sample1, pointB=sample2)\n",
    "        #print z_sample_one,'\\n'\n",
    "\n",
    "        g_images = sess.run(g_output,feed_dict={z_vector:z_sample_one})\n",
    "        g_image = g_images[0] # only retrieve the first returned image\n",
    "        \n",
    "        # save interpolated image\n",
    "        image = inverse_transform(g_image)\n",
    "        image = np.reshape(image,[32,32])\n",
    "        image = Image.fromarray(image)\n",
    "        scipy.misc.imsave(sample_inter_directory+'/'+str(i)+'.png', image)\n",
    "    \n",
    "    print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### turn the interpolate images into GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_names = sorted([int(f[:-4]) for f in os.listdir(sample_inter_directory) if f.endswith('.png')]) # get image filenames\n",
    "file_names = [str(f)+'.png' for f in file_names] # add the format suffix at the end of the filenames\n",
    "images = [Image.open(sample_inter_directory+f) for f in file_names] # turn into image instances\n",
    "filename = \"aa.gif\"\n",
    "writeGif(sample_inter_directory+filename, images, duration=0.1) # combine images to gif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
